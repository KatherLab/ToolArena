def medsss_generate(user_message: str = "How to stop a cough?") -> dict:
    """
    Given a user message, generate a response using the MedSSS_Policy model.

    Args:
        user_message: The user message.

    Returns:
        dict with the following structure:
        {
          'response': str  # The response generated by the model.
        }
    """

    # Adapted from https://github.com/pixas/MedSSS?tab=readme-ov-file#%EF%B8%8F-model
    from peft import PeftModel
    from transformers import AutoModelForCausalLM, AutoTokenizer

    base_model = AutoModelForCausalLM.from_pretrained(
        "meta-llama/Llama-3.1-8B-Instruct", torch_dtype="auto", device_map="auto"
    )
    model = PeftModel.from_pretrained(
        base_model, "pixas/MedSSS_Policy", torc_dtype="auto", device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained("pixas/MedSSS_Policy")
    messages = [{"role": "user", "content": user_message}]
    inputs = tokenizer(
        tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        ),
        return_tensors="pt",
    ).to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=2048)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {"response": response}
